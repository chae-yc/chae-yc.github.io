---
title: "OpenMPI"
categories:
  - scientist
  - HPC
tags:
  - open-mpi
last_modified_at: 2020-02-12T19:58:00+09:00
toc: true
published: true
share: false
---

## Structure of OpenMPI

![layers](/assets/images/2020-02-12-OpenMPI/open-mpi-layers.png)

- OPAL(Open, Portable Access Layer)
  - provides utility and glue code such as generic linked lists, string manipulation, debugging controls, and other mundane(평범한 functionality.
  - provides OPEN MPI's core portability such as discovering IP interfaces, sharing memory between processes on the same server, processor and memory affinity, high-precision timers, etc.
- ORTE(Open MPI Run-Time Environment; pronounced "or-tay")
  - An MPI implementation must provide not only the required message passing API, but also an accompanying run-time system to launch, monitor, and kill parallel jobs.
  - In Open MPI's case, a parallel job is comprised of one or more processes that may span multiple operating system instances, and are bound together to act as a single, cohesive unit.
  - In simple environments with little or no distributed computational support, ORTE uses rsh or ssh to launch the individual processes in parallel jobs. More advanced, HPC-dedicated environments typically have schedulers and resource managers for fairly sharing computational resources between many users.
  - Such environments usually provide specialized APIs to launch and regulate processes on compute servers.
  - ORTE supports a wide variety of such managed environments, such as (but not limited to): Torque/PBS Pro, SLURM, Oracle Grid Engine, and LSF.
- OMPI(Open MPI)
  - MPI API is implemented in this layer, as are all the message passing semantics defined by the MPI standard.
  - Since portability is a primary requirement, the MPI layer supports a wide variety of network types and underlying protocols.