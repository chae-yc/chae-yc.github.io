---
title: "Slurm - Overview"
categories:
  - scientist
  - slurm
  - Linux
tags:
  - cluster management
  - open source
  - job scheduling
last_modified_at: 2020-01-22T13:22:00+09:00
toc: true
published: true
share: false
---

> Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.

## Three key functions (cluster workload manager)

>First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.

- 작업을 수행할 수 있도록 일정 시간 동안 사용자에게 resource(compute node)에 대한 exclusive and/or non-exclusive access를 할당한다. 
- starting, executing, monitoring (normally a parallel job) 작업을 하는 프레임워크를 제공한다.
- pending work queue를 관리하여 작업들의 resource 사용을 관리한다.

## Optional Plugins

- accounting
- advanced reservation
- gang scheduling (time sharing for parallel jobs)
- backfill scheduling, topology optimized resource selection
- resource limits by user or bank account
- sophisticated multifactor job prioritization algorithms

등을 위해서 추가 플러그인을 사용할 수 있다.

## Architecture

![Slurm Architecture](/assets/images/2020-01-22-Slurm/arch.png){: width="40%"}

### slurmctld

**slurmctld** 는 중심에 있는 관리 서버에서 동작한다.
장애 발생을 대비해 backup manager 에도 동작 하기도 한다.

### slurmd

각 compute server(node)에는 remote shell과 비슷하다고 볼 수 있는(can be compared) **slurmd** deamon이 있고 아래의 기능을 담당한다.

1. waits for work
2. executes that work
3. returns status
4. waits for more work

**slurmd** deamon은 fault-tolerant hierarchical communications 를 제공한다.

### **slurmdbd** (Slurm database deamon)

 multiple Slurm-managed cluster들의 accrounting information을 하나의 데이터베이스에 저장할 수 있다.

### User tools
- srun: initiate job
- scancle: terminate queued or running jobs
- sinfo: report system status
- squeue: report the status of jobs
- sacct: get information about jobs and job steps
    - that are running or have completed
- smap, sview: graphically reports system and job status including network topology
- scontrol: monitor and/or modify configuration and state information on cluster
- sacctmgr: manage database
    -can identify clusters, valid users, valid bank accounts, etc.

### Plugins

- Accounting Storage
    - Primarily Used to store historical data about jobs. When used with SlurmDBD (Slurm Database Daemon), it can also supply a limits based system along with historical system status.
- Account Gather Energy
    - Gather energy consumption data per job or nodes in the system. This plugin is integrated with the Accounting Storage and Job Account Gather plugins.
- Authentication of communications
    - Provides authentication mechanism between various components of Slurm.
- Checkpoint
    - Interface to various checkpoint mechanisms.
- Containers
    - HPC workload container support and implementations.
- Credential (Digital Signature Generation)
    - Mechanism used to generate a digital signature, which is used to validate that job step is authorized to execute on specific nodes. This is distinct from the plugin used for Authentication since the job step request is sent from the user's srun command rather than directly from the slurmctld daemon, which generates the job step credential and its digital signature.
Generic Resources: Provide interface to control generic resources like Processing Units (GPUs) and Intel® Many Integrated Core (MIC) processors.
- Job Submit
    - Custom plugin to allow site specific control over job requirements at submission and update.
- Job Accounting Gather
    - Gather job step resource utilization data.
- Job Completion Logging
    - Log a job's termination data. This is typically a subset of data stored by an Accounting Storage Plugin.
- Launchers
    - Controls the mechanism used by the 'srun' command to launch the tasks.
- MPI
    - Provides different hooks for the various MPI implementations. For example, this can set MPI specific environment variables.
- Preempt
    - Determines which jobs can preempt other jobs and the preemption mechanism to be used.
- Priority
    - Assigns priorities to jobs upon submission and on an ongoing basis (e.g. as they age).
- Process tracking (for signaling)
    - Provides a mechanism for identifying the processes associated with each job. Used for job accounting and signaling.
Scheduler: Plugin determines how and when Slurm schedules jobs.
- Node selection
    - Plugin used to determine the resources used for a job allocation.
- Site Factor (Priority)
    - Assigns a specific site_factor component of a job's multifactor priority to jobs upon submission and on an ongoing basis (e.g. as they age).
- Switch or interconnect
    - Plugin to interface with a switch or interconnect. For most systems (ethernet or infiniband) this is not needed.
- Task Affinity
    - Provides mechanism to bind a job and it's individual tasks to specific processors.
- Network Topology
    - Optimizes resource selection based upon the network topology. Used for both job allocations and advanced reservation.

## Slurm for MPI and UPC asers

Slurm에서 MPI 를 사용하는 것은 어떤 MPI를 사용하는 것이냐에 따라서 3개의 모드로 나눌 수 있다.

1. Slurm이 task를 직접 launch하고 PIM2 나 PMIx API를 통해서 통신을 하는 것.
2. Slurm이 리소스를 할당하고 mpirun은 <u>Slurm 안에 있는 인프라</u>(이전 버전의 OpenMPI)를 사용하여 작업을 하는 것
3. Slurm이 리소스를 할당하고, mpirun은 Slurm이 아닌 SSH 나 RSH를 사용하여 작업을 하는 것.
작업이 Slurm 외부에서 이루어 지므로 작업 할당이 해제될 때 Slurm 에서도 이 작업들을 제거할 수 있도록 구성해줘야함. **pam_slurm_adopt**를 사용하는 것 추천.

> Distributions provide separate RPMs for Slurm’s PMIx support. If installing from source, note that an appropriate version of PMIx must be installed prior to building Slurm! More details are provided below.

> We recommend installing Slurm and PMIx in different (non-default) locations to avoid the conflict. 

> [pmix-slurm support](https://pmix.org/support/how-to/slurm-support/)


## Commands

If no jobs are currently running on the node:
scontrol update nodename={nodename} state={idle, resume etc.}
If jobs are running on the node:
scontrol update nodename=node10 state=resume



## Ref

[Slurm Document - Overview](https://slurm.schedmd.com/overview.html)

[Slurm Document - MPI and UPC Users Guide](https://slurm.schedmd.com/mpi_guide.html)

[PMI?](https://slurm.schedmd.com/SLUG15/PMIx.pdf)
